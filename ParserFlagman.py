import streamlit as st
import requests
from bs4 import BeautifulSoup
import json
import pandas as pd
import time
import random
import re
from io import BytesIO

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title="Flagman Monitor Pro", page_icon="üé£", layout="wide")

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Session State (–ë–ï–ó–û–ü–ê–°–ù–ê–Ø) ---
if 'all_links' not in st.session_state:
    st.session_state.all_links = []
if 'scraped_data' not in st.session_state:
    st.session_state.scraped_data = []
if 'found_categories' not in st.session_state:
    st.session_state.found_categories = []
if 'current_pos' not in st.session_state:
    st.session_state.current_pos = 1

# --- –§—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ ---

def get_soup(url, lang="uk"):
    cookies = {'i18n_redirected': lang}
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
        "Accept-Language": "ru-RU,ru;q=0.9" if lang == "ru" else "uk-UA,uk;q=0.9",
        "Referer": "https://flagman.ua/"
    }
    try:
        session = requests.Session()
        response = session.get(url, headers=headers, cookies=cookies, timeout=20)
        if response.status_code == 200:
            return BeautifulSoup(response.text, "lxml")
    except:
        return None

def get_subcategories_with_names(soup):
    sub_data = []
    items = soup.select("a.item-link")
    for link in items:
        name_tag = link.select_one(".fish-title-mobile") or link.select_one(".category-name") or link
        name = name_tag.get_text(strip=True)
        href = link.get("href")
        if href and "/c" in href and name:
            if not href.startswith("http"): href = "https://flagman.ua" + href
            url = href.replace("/ru/", "/")
            sub_data.append({"name": name, "url": url})
    unique = {v['url']: v for v in sub_data}.values()
    return list(unique)

def get_product_links(cat_url, max_pages):
    links = []
    page = 1
    while True:
        if max_pages and page > max_pages: break
        page_url = f"{cat_url}/page={page}" if page > 1 else cat_url
        soup = get_soup(page_url)
        if not soup: break
        scripts = soup.find_all("script", type="application/ld+json")
        page_links = []
        for script in scripts:
            try:
                data = json.loads(script.string)
                if data.get("@type") == "ItemList":
                    for element in data.get("itemListElement", []):
                        p_url = element.get("item", {}).get("url")
                        if p_url: page_links.append(p_url)
            except: continue
        if not page_links: break
        links.extend(page_links)
        page += 1
        time.sleep(0.1)
    return list(dict.fromkeys(links))

def parse_page_content(soup):
    if not soup: return "N/A", "N/A", "N/A", {}, {}
    product_json = {}
    scripts = soup.find_all("script", type="application/ld+json")
    for script in scripts:
        try:
            js_data = json.loads(script.string)
            if isinstance(js_data, dict) and js_data.get("@type") == "Product":
                product_json = js_data
                break
        except: continue
    
    title_tag = soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else product_json.get("name", "N/A")
    desc_block = soup.select_one(".product-description-text") or soup.select_one(".product-description__content")
    
    description_clean = desc_block.get_text(separator="\n", strip=True) if desc_block else ""
    description_html = desc_block.decode_contents().strip() if desc_block else ""
    
    chars = {}
    char_items = soup.select(".chars-items-wrapper .chars-item") or soup.select(".product-properties__item")
    for ci in char_items:
        p_tags = ci.find_all("p")
        if len(p_tags) >= 2:
            chars[p_tags[0].get_text(strip=True)] = p_tags[1].get_text(strip=True)
            
    return title, description_clean, description_html, chars, product_json

# --- –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ---

st.title("üé£ Flagman Smart Monitor Pro+")

with st.sidebar:
    st.header("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö")
    if st.button("üóë –ü–æ–ª–Ω—ã–π —Å–±—Ä–æ—Å"):
        for key in st.session_state.keys():
            del st.session_state[key]
        st.rerun()
    
    st.write(f"–¢–æ–≤–∞—Ä–æ–≤ –≤ —Ç–∞–±–ª–∏—Ü–µ: **{len(st.session_state.scraped_data)}**")

# –®–∞–≥ 1
st.subheader("1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
c1, c2 = st.columns([3, 1])
with c1:
    input_url = st.text_input("–°—Å—ã–ª–∫–∞ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é", placeholder="https://flagman.ua/...")
with c2:
    pages_limit = st.number_input("–°—Ç—Ä. (0=–≤—Å–µ)", min_value=0, value=1)

if st.button("üîç –ù–∞–π—Ç–∏ —Ä–∞–∑–¥–µ–ª—ã"):
    if input_url:
        base_url = input_url.replace("/ru/", "/")
        soup_main = get_soup(base_url)
        found = get_subcategories_with_names(soup_main)
        st.session_state.found_categories = found if found else [{"name": "–¢–µ–∫—É—â–∏–π —Ä–∞–∑–¥–µ–ª", "url": base_url}]
        st.rerun()

# –®–∞–≥ 2
if st.session_state.found_categories:
    st.subheader("2. –í—ã–±–æ—Ä –ø–æ–¥—Ä–∞–∑–¥–µ–ª–æ–≤")
    cat_map = {c['name']: c['url'] for c in st.session_state.found_categories}
    selected_cats = st.multiselect("–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å:", options=list(cat_map.keys()), default=list(cat_map.keys()))
    
    if st.button("üîé –°–æ–±—Ä–∞—Ç—å —Å—Å—ã–ª–∫–∏"):
        all_links_list = []
        with st.spinner("–°–±–æ—Ä –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫..."):
            for name in selected_cats:
                links = get_product_links(cat_map[name], None if pages_limit == 0 else pages_limit)
                all_links_list.extend(links)
            st.session_state.all_links = list(dict.fromkeys(all_links_list))
        st.rerun()

# –®–∞–≥ 3
if st.session_state.all_links:
    total_q = len(st.session_state.all_links)
    st.subheader("3. –§–∏–ª—å—Ç—Ä—ã –∏ –∑–∞–ø—É—Å–∫")
    
    c_sk, c_ht = st.columns([2, 1])
    with c_sk:
        skus_raw = st.text_area("–°–ø–∏—Å–æ–∫ –ê—Ä—Ç–∏–∫—É–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ):", height=100)
    with c_ht:
        clean_html_flag = st.checkbox("–û—á–∏—â–∞—Ç—å HTML —Ç–µ–≥–∏", value=True)
    
    target_skus = [x.strip() for x in re.split(r'[,\n\s]+', skus_raw) if x.strip()] if skus_raw else []

    # –ò–ù–§–û –ü–ê–ù–ï–õ–¨
    st.info(f"üìã –û—á–µ—Ä–µ–¥—å: **{total_q}** | üìç –ü–æ–∑–∏—Ü–∏—è: **{st.session_state.current_pos}** | ‚úÖ –ù–∞–π–¥–µ–Ω–æ: **{len(st.session_state.scraped_data)}**")
    
    col_f, col_c, col_g = st.columns([1, 1, 2])
    with col_f:
        start_val = min(st.session_state.current_pos, total_q)
        start_idx = st.number_input("–ù–∞—á–∞—Ç—å —Å ‚Ññ", min_value=1, max_value=total_q+1, value=start_val)
    with col_c:
        batch_size = st.number_input("–ö–æ–ª-–≤–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏", min_value=1, max_value=2000, value=100)
    
    if col_g.button("üöÄ –ó–ê–ü–£–°–¢–ò–¢–¨ –ü–ê–†–°–ò–ù–ì"):
        end_idx = min(int(start_idx) + int(batch_size) - 1, total_q)
        work_links = st.session_state.all_links[int(start_idx)-1 : end_idx]
        
        progress_bar = st.progress(0)
        st_info = st.empty()
        skip_keys = ["–ö–æ–¥ —Ç–æ–≤–∞—Ä—É", "–ö–æ–¥ —Ç–æ–≤–∞—Ä–∞", "–ê—Ä—Ç–∏–∫—É–ª", "–ê—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä—É", "–í–∏—Ä–æ–±–Ω–∏–∫", "–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å"]

        for i, link in enumerate(work_links):
            curr_num = int(start_idx) + i
            st_info.write(f"üîπ –ü—Ä–æ–≤–µ—Ä–∫–∞ **{curr_num} –∏–∑ {total_q}**...")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ UA
            soup_ua = get_soup(link.replace("/ru/", "/"), "uk")
            if not soup_ua: continue
            
            t_ua, d_ua_cl, d_ua_rw, c_ua, j_ua = parse_page_content(soup_ua)
            sku = j_ua.get("sku", "N/A")

            # –§–∏–ª—å—Ç—Ä –ø–æ –∞—Ä—Ç–∏–∫—É–ª–∞–º
            if target_skus and sku not in target_skus:
                progress_bar.progress((i + 1) / len(work_links))
                continue

            # –ï—Å–ª–∏ –ø—Ä–æ—à–µ–ª —Ñ–∏–ª—å—Ç—Ä –∏–ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞ –Ω–µ—Ç - –ø–∞—Ä—Å–∏–º RU
            st_info.write(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ! –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö: **{sku}**")
            soup_ru = get_soup(link.replace("flagman.ua/", "flagman.ua/ru/"), "ru")
            t_ru, d_ru_cl, d_ru_rw, c_ru, j_ru = parse_page_content(soup_ru)
            
            # –ß–∏—Å—Ç–∫–∞ —Ñ–æ—Ç–æ
            imgs = [img.get('src') for img in soup_ua.select(".product-images img") 
                    if img.get('src') and not img.get('src').startswith("data:image")]
            
            # –°–±–æ—Ä —Å—Ç—Ä–æ–∫–∏
            row = {
                "–ê—Ä—Ç–∏–∫—É–ª": sku,
                "–ë—Ä–µ–Ω–¥": j_ua.get("brand", {}).get("name", "N/A"),
                "–¶–µ–Ω–∞": j_ua.get("offers", {}).get("price", "N/A"),
                "–ù–∞–∑–≤–∞ (UA)": t_ua, "–ù–∞–∑–≤–∞–Ω–∏–µ (RU)": t_ru,
                "–û–ø–∏—Å (UA)": d_ua_cl if clean_html_flag else d_ua_rw,
                "–û–ø–∏—Å–∞–Ω–∏–µ (RU)": d_ru_cl if clean_html_flag else d_ru_rw
            }
            # –§–æ—Ç–æ –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º
            for idx, url in enumerate(imgs[:15]): row[f"–§–æ—Ç–æ {idx+1}"] = url
            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            for k, v in c_ua.items():
                if k not in skip_keys: row[f"{k} (UA)"] = v
            for k, v in c_ru.items():
                if k not in skip_keys: row[f"{k} (RU)"] = v

            row["–°—Å—ã–ª–∫–∞ (UA)"] = link.replace("/ru/", "/")
            row["–°—Å—ã–ª–∫–∞ (RU)"] = link.replace("flagman.ua/", "flagman.ua/ru/")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (–±–µ–∑ –¥—É–±–ª–µ–π –≤ –ø–∞–º—è—Ç–∏)
            if not any(d['–ê—Ä—Ç–∏–∫—É–ª'] == sku for d in st.session_state.scraped_data):
                st.session_state.scraped_data.append(row)
            
            progress_bar.progress((i + 1) / len(work_links))
            time.sleep(0.05) # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –∏ –ü–ï–†–ï–ó–ê–ì–†–£–ñ–ê–ï–ú —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        st.session_state.current_pos = min(end_idx + 1, total_q)
        st_info.empty()
        st.rerun()

# –®–∞–≥ 4
if st.session_state.scraped_data:
    st.subheader("4. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    df = pd.DataFrame(st.session_state.scraped_data)
    st.dataframe(df.head(5))
    
    out = BytesIO()
    with pd.ExcelWriter(out, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name='Flagman', index=False)
    
    st.download_button(f"üì• –°–∫–∞—á–∞—Ç—å Excel ({len(df)} —Ç–æ–≤–∞—Ä–æ–≤)", data=out.getvalue(), 
                       file_name="flagman_report.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
