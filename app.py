import streamlit as st
import requests
from bs4 import BeautifulSoup
import json
import pandas as pd
import time
from io import BytesIO

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title="Flagman Parser", page_icon="üé£")

def get_soup(url, lang="uk"):
    cookies = {'i18n_redirected': lang}
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Accept-Language": "ru-RU,ru;q=0.9" if lang == "ru" else "uk-UA,uk;q=0.9"
    }
    try:
        response = requests.get(url, headers=headers, cookies=cookies, timeout=20)
        if response.status_code == 200:
            return BeautifulSoup(response.text, "lxml")
    except:
        return None

def get_product_links(cat_url, lang, max_pages):
    links = []
    page = 1
    while True:
        if max_pages and page > max_pages: break
        page_url = f"{cat_url}/page={page}" if page > 1 else cat_url
        soup = get_soup(page_url, lang=lang)
        if not soup: break
        
        scripts = soup.find_all("script", type="application/ld+json")
        page_links = []
        for script in scripts:
            try:
                data = json.loads(script.string)
                if data.get("@type") == "ItemList":
                    for element in data.get("itemListElement", []):
                        p_url = element.get("item", {}).get("url")
                        if p_url: page_links.append(p_url)
            except: continue
        
        if not page_links: break
        links.extend(page_links)
        page += 1
        time.sleep(0.5)
    return list(dict.fromkeys(links))

def get_product_details(url, lang):
    soup = get_soup(url, lang=lang)
    if not soup: return None
    
    product_json = {}
    scripts = soup.find_all("script", type="application/ld+json")
    for script in scripts:
        try:
            js_data = json.loads(script.string)
            if isinstance(js_data, dict) and js_data.get("@type") == "Product":
                product_json = js_data
                break
        except: continue

    title_tag = soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else product_json.get("name", "N/A")
    desc_block = soup.select_one(".product-description-text") or soup.select_one(".product-description__content")
    description = desc_block.get_text(separator="\n", strip=True) if desc_block else ""

    item = {
        "–ù–∞–∑–≤–∞–Ω–∏–µ" if lang == "ru" else "–ù–∞–∑–≤–∞": title,
        "–ê—Ä—Ç–∏–∫—É–ª": product_json.get("sku"),
        "–¶–µ–Ω–∞" if lang == "ru" else "–¶—ñ–Ω–∞": product_json.get("offers", {}).get("price"),
        "–û–ø–∏—Å–∞–Ω–∏–µ" if lang == "ru" else "–û–ø–∏—Å": description,
        "–§–æ—Ç–æ": " ".join([img.get('src') for img in soup.select(".product-images img") if img.get('src')]),
        "–°—Å—ã–ª–∫–∞": url
    }

    for char in (soup.select(".chars-items-wrapper .chars-item") or soup.select(".product-properties__item")):
        names = char.find_all("p")
        if len(names) >= 2:
            item[names[0].get_text(strip=True)] = names[1].get_text(strip=True)
    return item

# --- –ò–ù–¢–ï–†–§–ï–ô–° –°–¢–†–ò–ú–õ–ò–¢ ---
st.title("üé£ Flagman Parser PRO")
st.write("–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é, –∏ —è —Å–æ–±–µ—Ä—É –¥–∞–Ω–Ω—ã–µ –¥–ª—è UA –∏ RU –≤–µ—Ä—Å–∏–π.")

input_url = st.text_input("–°—Å—ã–ª–∫–∞ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é", placeholder="https://flagman.ua/...")
pages_limit = st.number_input("–ö–æ–ª-–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü (0 = –≤—Å–µ)", min_value=0, value=1)

if st.button("–ù–∞—á–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥"):
    if not input_url:
        st.error("–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫—É!")
    else:
        # –õ–æ–≥–∏–∫–∞ —Å—Å—ã–ª–æ–∫
        ua_url = input_url.replace("/ru/", "/")
        ru_url = input_url if "/ru/" in input_url else input_url.replace("flagman.ua/", "flagman.ua/ru/")
        max_p = None if pages_limit == 0 else pages_limit

        with st.status("–†–∞–±–æ—Ç–∞—é...", expanded=True) as status:
            st.write("–°–æ–±–∏—Ä–∞—é —Å—Å—ã–ª–∫–∏...")
            links_ua = get_product_links(ua_url, "uk", max_p)
            links_ru = get_product_links(ru_url, "ru", max_p)
            
            all_links = list(set(links_ua + links_ru))
            st.write(f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(all_links)}")
            
            data_ua, data_ru = [], []
            progress_bar = st.progress(0)
            
            for i, link in enumerate(all_links):
                # –î–ª—è UA
                d_ua = get_product_details(link.replace("/ru/", "/"), "uk")
                if d_ua: data_ua.append(d_ua)
                # –î–ª—è RU
                d_ru = get_product_details(link if "/ru/" in link else link.replace("flagman.ua/", "flagman.ua/ru/"), "ru")
                if d_ru: data_ru.append(d_ru)
                
                progress_bar.progress((i + 1) / len(all_links))
                time.sleep(0.5)

            status.update(label="–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!", state="complete")

        # –°–æ–∑–¥–∞–Ω–∏–µ Excel –≤ –ø–∞–º—è—Ç–∏
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            pd.DataFrame(data_ua).to_excel(writer, sheet_name='UA', index=False)
            pd.DataFrame(data_ru).to_excel(writer, sheet_name='RU', index=False)
        
        st.success("–¢–∞–±–ª–∏—Ü–∞ –≥–æ—Ç–æ–≤–∞!")
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å Excel —Ä–µ–∑—É–ª—å—Ç–∞—Ç",
            data=output.getvalue(),
            file_name="flagman_export.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
