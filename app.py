import streamlit as st
import requests
from bs4 import BeautifulSoup
import json
import pandas as pd
import time
from io import BytesIO

st.set_page_config(page_title="Flagman Deep Parser", page_icon="üé£")

def get_soup(url, lang="uk"):
    cookies = {'i18n_redirected': lang}
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Accept-Language": "ru-RU,ru;q=0.9" if lang == "ru" else "uk-UA,uk;q=0.9"
    }
    try:
        session = requests.Session()
        response = session.get(url, headers=headers, cookies=cookies, timeout=20)
        if response.status_code == 200:
            return BeautifulSoup(response.text, "lxml")
    except:
        return None

def get_subcategories(soup):
    """–ò—â–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    sub_links = []
    # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ –±–ª–æ–∫–∞—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–æ–±—ã—á–Ω–æ –æ–Ω–∏ –∏–º–µ—é—Ç –∫–ª–∞—Å—Å .item-link –∏–ª–∏ –ª–µ–∂–∞—Ç –≤–Ω—É—Ç—Ä–∏ –ø–ª–∏—Ç–∫–∏)
    cat_grid = soup.select("a.item-link")
    for link in cat_grid:
        href = link.get("href")
        if href and "/c" in href: # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            if not href.startswith("http"):
                href = "https://flagman.ua" + href
            sub_links.append(href.replace("/ru/", "/"))
    return list(dict.fromkeys(sub_links))

def get_product_links(cat_url, max_pages):
    links = []
    page = 1
    while True:
        if max_pages and page > max_pages: break
        page_url = f"{cat_url}/page={page}" if page > 1 else cat_url
        soup = get_soup(page_url)
        if not soup: break
        
        scripts = soup.find_all("script", type="application/ld+json")
        page_links = []
        for script in scripts:
            try:
                data = json.loads(script.string)
                if data.get("@type") == "ItemList":
                    for element in data.get("itemListElement", []):
                        p_url = element.get("item", {}).get("url")
                        if p_url: page_links.append(p_url)
            except: continue
        
        if not page_links: break
        links.extend(page_links)
        page += 1
        time.sleep(0.3)
    return list(dict.fromkeys(links))

def parse_page_content(soup):
    if not soup: return "N/A", "N/A", {}
    product_json = {}
    scripts = soup.find_all("script", type="application/ld+json")
    for script in scripts:
        try:
            js_data = json.loads(script.string)
            if isinstance(js_data, dict) and js_data.get("@type") == "Product":
                product_json = js_data
                break
        except: continue

    title_tag = soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else product_json.get("name", "N/A")
    desc_block = soup.select_one(".product-description-text") or soup.select_one(".product-description__content")
    description = desc_block.get_text(separator="\n", strip=True) if desc_block else ""
    
    chars = {}
    char_items = soup.select(".chars-items-wrapper .chars-item") or soup.select(".product-properties__item")
    for ci in char_items:
        p_tags = ci.find_all("p")
        if len(p_tags) >= 2:
            chars[p_tags[0].get_text(strip=True)] = p_tags[1].get_text(strip=True)
            
    return title, description, chars, product_json

# --- –ò–ù–¢–ï–†–§–ï–ô–° ---
st.title("üé£ Flagman Deep Parser")
st.write("–ú–æ–∂–Ω–æ –≤–≤–æ–¥–∏—Ç—å –∫–∞–∫ —Å—Å—ã–ª–∫—É –Ω–∞ **–ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é**, —Ç–∞–∫ –∏ –Ω–∞ **–≥–ª–∞–≤–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é** —Å –ø–ª–∏—Ç–∫–∞–º–∏.")

input_url = st.text_input("–°—Å—ã–ª–∫–∞ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é", placeholder="https://flagman.ua/ru/kotushky/c166336")
pages_limit = st.number_input("–ö–æ–ª-–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –∫–∞–∂–¥–æ–º –ø–æ–¥—Ä–∞–∑–¥–µ–ª–µ (0 = –≤—Å–µ)", min_value=0, value=1)

if st.button("–ù–∞—á–∞—Ç—å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"):
    if not input_url:
        st.error("–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫—É!")
    else:
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ UA –≤–∏–¥—É –¥–ª—è –ø–æ–∏—Å–∫–∞
        base_url = input_url.replace("/ru/", "/")
        soup_main = get_soup(base_url)
        
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç—É—Ç –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        sub_cats = get_subcategories(soup_main)
        
        if sub_cats:
            st.warning(f"–≠—Ç–æ –≥–ª–∞–≤–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è. –ù–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª–æ–≤: {len(sub_cats)}")
            target_categories = sub_cats
        else:
            st.info("–≠—Ç–æ –ø—Ä—è–º–∞—è –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è. –ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä —Ç–æ–≤–∞—Ä–æ–≤.")
            target_categories = [base_url]

        final_data = []
        skip_keys = ["–ö–æ–¥ —Ç–æ–≤–∞—Ä—É", "–ö–æ–¥ —Ç–æ–≤–∞—Ä–∞", "–ê—Ä—Ç–∏–∫—É–ª", "–ê—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä—É"]
        
        for cat_url in target_categories:
            st.write(f"üìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–¥–µ–ª–∞: {cat_url.split('/')[-2]}")
            
            product_links = get_product_links(cat_url, None if pages_limit == 0 else pages_limit)
            
            if not product_links:
                st.write("  - –¢–æ–≤–∞—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞—é.")
                continue

            # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö
            bar = st.progress(0)
            for i, link in enumerate(product_links):
                ua_link = link.replace("/ru/", "/")
                ru_link = link.replace("flagman.ua/", "flagman.ua/ru/")
                
                soup_ua = get_soup(ua_link, "uk")
                time.sleep(0.1)
                soup_ru = get_soup(ru_link, "ru")
                
                title_ua, desc_ua, chars_ua, json_ua = parse_page_content(soup_ua)
                title_ru, desc_ru, chars_ru, json_ru = parse_page_content(soup_ru)
                
                sku = json_ua.get("sku", "N/A")
                price = json_ua.get("offers", {}).get("price", "N/A")
                brand = json_ua.get("brand", {}).get("name", "N/A")
                
                img_tags = soup_ua.select(".product-images img")
                image_urls = [img.get('src') for img in img_tags if img.get('src')]
                
                row = {
                    "–ê—Ä—Ç–∏–∫—É–ª": sku,
                    "–ë—Ä–µ–Ω–¥": brand,
                    "–¶–µ–Ω–∞": price,
                    "–ù–∞–∑–≤–∞ (UA)": title_ua,
                    "–ù–∞–∑–≤–∞–Ω–∏–µ (RU)": title_ru,
                    "–û–ø–∏—Å (UA)": desc_ua,
                    "–û–ø–∏—Å–∞–Ω–∏–µ (RU)": desc_ru
                }
                for idx, img_url in enumerate(image_urls[:15]): row[f"–§–æ—Ç–æ {idx+1}"] = img_url
                for k, v in chars_ua.items():
                    if k not in skip_keys: row[f"{k} (UA)"] = v
                for k, v in chars_ru.items():
                    if k not in skip_keys: row[f"{k} (RU)"] = v

                row["–°—Å—ã–ª–∫–∞ (UA)"] = ua_link
                row["–°—Å—ã–ª–∫–∞ (RU)"] = ru_link
                final_data.append(row)
                bar.progress((i + 1) / len(product_links))
                time.sleep(0.5)

        if final_data:
            df = pd.DataFrame(final_data)
            output = BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='Flagman Data', index=False)
            
            st.success(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ! –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(final_data)}")
            st.download_button(
                label="üì• –°–∫–∞—á–∞—Ç—å –ø–æ–ª–Ω—ã–π Excel",
                data=output.getvalue(),
                file_name="flagman_deep_export.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        else:
            st.error("–î–∞–Ω–Ω—ã–µ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å.")
