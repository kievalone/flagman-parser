import streamlit as st
import requests
from bs4 import BeautifulSoup
import json
import pandas as pd
import time
from io import BytesIO

st.set_page_config(page_title="Flagman Parser Elite", page_icon="üé£")

# --- –§–£–ù–ö–¶–ò–ò –ü–ê–†–°–ò–ù–ì–ê ---

def get_soup(url, lang="uk"):
    cookies = {'i18n_redirected': lang}
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Accept-Language": "ru-RU,ru;q=0.9" if lang == "ru" else "uk-UA,uk;q=0.9"
    }
    try:
        session = requests.Session()
        response = session.get(url, headers=headers, timeout=20)
        if response.status_code == 200:
            return BeautifulSoup(response.text, "lxml")
    except:
        return None

def get_subcategories_with_names(soup):
    """–ò—â–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    sub_data = []
    # –ò—â–µ–º –±–ª–æ–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π. –û–±—ã—á–Ω–æ —ç—Ç–æ —Å—Å—ã–ª–∫–∏ —Å –∫–ª–∞—Å—Å–æ–º item-link
    items = soup.select("a.item-link")
    for link in items:
        name_tag = link.select_one(".fish-title-mobile") or link.select_one(".category-name") or link
        name = name_tag.get_text(strip=True)
        href = link.get("href")
        if href and "/c" in href and name:
            if not href.startswith("http"):
                href = "https://flagman.ua" + href
            url = href.replace("/ru/", "/")
            sub_data.append({"name": name, "url": url})
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_data = {v['url']: v for v in sub_data}.values()
    return list(unique_data)

def get_product_links(cat_url, max_pages):
    links = []
    page = 1
    while True:
        if max_pages and page > max_pages: break
        page_url = f"{cat_url}/page={page}" if page > 1 else cat_url
        soup = get_soup(page_url)
        if not soup: break
        
        scripts = soup.find_all("script", type="application/ld+json")
        page_links = []
        for script in scripts:
            try:
                data = json.loads(script.string)
                if data.get("@type") == "ItemList":
                    for element in data.get("itemListElement", []):
                        p_url = element.get("item", {}).get("url")
                        if p_url: page_links.append(p_url)
            except: continue
        
        if not page_links: break
        links.extend(page_links)
        page += 1
        time.sleep(0.2)
    return list(dict.fromkeys(links))

def parse_page_content(soup):
    if not soup: return "N/A", "N/A", {}
    product_json = {}
    scripts = soup.find_all("script", type="application/ld+json")
    for script in scripts:
        try:
            js_data = json.loads(script.string)
            if isinstance(js_data, dict) and js_data.get("@type") == "Product":
                product_json = js_data
                break
        except: continue

    title_tag = soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else product_json.get("name", "N/A")
    desc_block = soup.select_one(".product-description-text") or soup.select_one(".product-description__content")
    description = desc_block.get_text(separator="\n", strip=True) if desc_block else ""
    
    chars = {}
    char_items = soup.select(".chars-items-wrapper .chars-item") or soup.select(".product-properties__item")
    for ci in char_items:
        p_tags = ci.find_all("p")
        if len(p_tags) >= 2:
            chars[p_tags[0].get_text(strip=True)] = p_tags[1].get_text(strip=True)
            
    return title, description, chars, product_json

# --- –ò–ù–¢–ï–†–§–ï–ô–° STREAMLIT ---

st.title("üé£ Flagman Smart Monitor")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
if 'categories' not in st.session_state:
    st.session_state.categories = []

input_url = st.text_input("–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é (–≥–ª–∞–≤–Ω—É—é –∏–ª–∏ –≤–ª–æ–∂–µ–Ω–Ω—É—é)", 
                         placeholder="https://flagman.ua/ru/kotushky/c166336")

col1, col2 = st.columns(2)
with col1:
    pages_limit = st.number_input("–°—Ç—Ä–∞–Ω–∏—Ü –≤ –∫–∞–∂–¥–æ–º —Ä–∞–∑–¥–µ–ª–µ (0 = –≤—Å–µ)", min_value=0, value=1)
with col2:
    btn_find = st.button("üîç –ù–∞–π—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ / –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Å—ã–ª–∫—É")

# –®–∞–≥ 1: –ü–æ–∏—Å–∫ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π
if btn_find:
    if not input_url:
        st.error("–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫—É!")
    else:
        with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–∞–π—Ç–∞..."):
            base_url = input_url.replace("/ru/", "/")
            soup_main = get_soup(base_url)
            found_cats = get_subcategories_with_names(soup_main)
            
            if found_cats:
                st.session_state.categories = found_cats
                st.success(f"–ù–∞–π–¥–µ–Ω–æ —Ä–∞–∑–¥–µ–ª–æ–≤: {len(found_cats)}")
            else:
                # –ï—Å–ª–∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–µ—Ç, –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –ø—Ä—è–º–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
                st.session_state.categories = [{"name": "–¢–µ–∫—É—â–∏–π —Ä–∞–∑–¥–µ–ª (–±–µ–∑ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π)", "url": base_url}]
                st.info("–í–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ —Ç–µ–∫—É—â–∞—è —Å—Å—ã–ª–∫–∞.")

# –®–∞–≥ 2: –í—ã–±–æ—Ä –∏ –ó–∞–ø—É—Å–∫
if st.session_state.categories:
    st.write("### üìÇ –í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞:")
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –º—É–ª—å—Ç–∏—Å–µ–ª–µ–∫—Ç–∞
    cat_options = {c['name']: c['url'] for c in st.session_state.categories}
    selected_names = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –Ω—É–∂–Ω—ã–µ:", 
                                   options=list(cat_options.keys()), 
                                   default=list(cat_options.keys()))

    if st.button("üöÄ –ó–ê–ü–£–°–¢–ò–¢–¨ –ú–û–ù–ò–¢–û–†–ò–ù–ì"):
        if not selected_names:
            st.warning("–í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –∫–∞—Ç–µ–≥–æ—Ä–∏—é!")
        else:
            final_data = []
            skip_keys = ["–ö–æ–¥ —Ç–æ–≤–∞—Ä—É", "–ö–æ–¥ —Ç–æ–≤–∞—Ä–∞", "–ê—Ä—Ç–∏–∫—É–ª", "–ê—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä—É"]
            
            total_selected = len(selected_names)
            
            for c_idx, name in enumerate(selected_names):
                cat_url = cat_options[name]
                st.write(f"---")
                st.write(f"üì¶ **–†–∞–∑–¥–µ–ª [{c_idx+1}/{total_selected}]: {name}**")
                
                # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã
                status_text = st.empty()
                status_text.write("üîé –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã...")
                product_links = get_product_links(cat_url, None if pages_limit == 0 else pages_limit)
                
                if not product_links:
                    st.write("‚ùå –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–µ—Ç.")
                    continue
                
                total_links = len(product_links)
                st.write(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {total_links}")
                
                # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π —Ç–æ–≤–∞—Ä
                bar = st.progress(0)
                item_status = st.empty()
                
                for i, link in enumerate(product_links):
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
                    item_status.write(f"üîπ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–≤–∞—Ä–∞ **{i+1} –∏–∑ {total_links}**")
                    
                    ua_link = link.replace("/ru/", "/")
                    ru_link = link.replace("flagman.ua/", "flagman.ua/ru/")
                    
                    soup_ua = get_soup(ua_link, "uk")
                    time.sleep(0.1)
                    soup_ru = get_soup(ru_link, "ru")
                    
                    title_ua, desc_ua, chars_ua, json_ua = parse_page_content(soup_ua)
                    title_ru, desc_ru, chars_ru, json_ru = parse_page_content(soup_ru)
                    
                    sku = json_ua.get("sku", "N/A")
                    price = json_ua.get("offers", {}).get("price", "N/A")
                    brand = json_ua.get("brand", {}).get("name", "N/A")
                    
                    image_urls = [img.get('src') for img in soup_ua.select(".product-images img") if img.get('src')]
                    
                    row = {
                        "–ê—Ä—Ç–∏–∫—É–ª": sku,
                        "–ë—Ä–µ–Ω–¥": brand,
                        "–¶–µ–Ω–∞": price,
                        "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": name,
                        "–ù–∞–∑–≤–∞ (UA)": title_ua,
                        "–ù–∞–∑–≤–∞–Ω–∏–µ (RU)": title_ru,
                        "–û–ø–∏—Å (UA)": desc_ua,
                        "–û–ø–∏—Å–∞–Ω–∏–µ (RU)": desc_ru
                    }
                    for idx, img_url in enumerate(image_urls[:15]): row[f"–§–æ—Ç–æ {idx+1}"] = img_url
                    for k, v in chars_ua.items():
                        if k not in skip_keys: row[f"{k} (UA)"] = v
                    for k, v in chars_ru.items():
                        if k not in skip_keys: row[f"{k} (RU)"] = v

                    row["–°—Å—ã–ª–∫–∞ (UA)"] = ua_link
                    row["–°—Å—ã–ª–∫–∞ (RU)"] = ru_link
                    final_data.append(row)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
                    bar.progress((i + 1) / total_links)
                    time.sleep(0.3)
                
                item_status.empty()

            if final_data:
                df = pd.DataFrame(final_data)
                output = BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    df.to_excel(writer, sheet_name='Flagman Data', index=False)
                
                st.balloons()
                st.success(f"üíé –í–°–Å –ì–û–¢–û–í–û! –°–æ–±—Ä–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(final_data)}")
                st.download_button(
                    label="üì• –°–ö–ê–ß–ê–¢–¨ EXCEL –¢–ê–ë–õ–ò–¶–£",
                    data=output.getvalue(),
                    file_name="flagman_full_monitoring.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
